{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Landmark_Recognition_Main_Project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZAAAhKbA8db"
      },
      "source": [
        "# Loading the dataset from google drive into google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHPd-FD4Wf0o"
      },
      "source": [
        "# Modules which used in the project\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras import layers\n",
        "from urllib.request import urlopen\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import traceback\n",
        "import pathlib\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siqscVlDWf05"
      },
      "source": [
        "# Reading the train.csv file and called the head function to load few lines of data.\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Landmark_Recognition_Project/train.csv')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3bN8rQyWf08"
      },
      "source": [
        "# number of rows and columns in the dataframe\n",
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QultOr0dWf0-"
      },
      "source": [
        "# Feature data types\n",
        "train_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSe6lPbPWf0_"
      },
      "source": [
        "# Looking for null values in the dataset\n",
        "train_df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dqqd3uyWf1E"
      },
      "source": [
        "train_df.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe0BdsUtWf1I"
      },
      "source": [
        "# Now try to open the URL\n",
        "temp = 4444\n",
        "print('id', train_df['id'][temp])\n",
        "print('url:', train_df['url'][temp])\n",
        "print('landmark id:', train_df['landmark_id'][temp])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3dtkCimWf1J"
      },
      "source": [
        "# Cleaning the dataset as found out there are some 'NONE' strings in the urls. \n",
        "dropped_rows = train_df.loc[train_df['url'] == 'None'].index\n",
        "dropped_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuNPopUVWf1L"
      },
      "source": [
        "# Dropped the 'NONE' urls from the dataset\n",
        "train_df.drop(dropped_rows, inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JalJbtT9Wf1L"
      },
      "source": [
        "# Trying to find out the unique landmark_id in the dataset\n",
        "train_df['landmark_id'].value_counts().head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEXS7ZvGWf1M"
      },
      "source": [
        "# Occurance of landmark_id in decreasing order(Top categories)\n",
        "temp = pd.DataFrame(train_df.landmark_id.value_counts().head(13))\n",
        "temp.reset_index(inplace=True)\n",
        "temp.columns = ['landmark_id','count']\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7XYNMzfWf1Q"
      },
      "source": [
        "# Plot the most frequent landmark_ids\n",
        "plt.figure(figsize = (9, 8))\n",
        "plt.title('Most frequent landmarks')\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.barplot(x=\"landmark_id\", y=\"count\", data=temp,\n",
        "            label=\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vj4ThMUWf1S"
      },
      "source": [
        "# Occurance of landmark_id in increasing order\n",
        "temp = pd.DataFrame(train_df.landmark_id.value_counts().tail(13))\n",
        "temp.reset_index(inplace=True)\n",
        "temp.columns = ['landmark_id','count']\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AyBw0gtWf1Z"
      },
      "source": [
        "# Plot the least frequent landmark_ids\n",
        "plt.figure(figsize = (9, 8))\n",
        "plt.title('Least frequent landmarks')\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.barplot(x=\"landmark_id\", y=\"count\", data=temp,\n",
        "            label=\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRpRRqclWf1b"
      },
      "source": [
        "# Mapping which landmark_id refers to the landmark.\n",
        "\n",
        "key_map = { \n",
        "    \"9633\": \"san_pietro_vatican_city\",\n",
        "    \"6051\": \"colosseum_rome\",\n",
        "    \"9779\": \"el_partal_spain\", \n",
        "    \"2061\": \"powder_tower_prague_czech\", \n",
        "    \"5554\": \"petronas_towers_malaysia\",\n",
        "    \"5376\": \"rialto_bridge_venice\",\n",
        "    \"6696\": \"national_museum_of_catalunya\",\n",
        "    \"2743\": \"pantheon_rome\",\n",
        "    \"4352\": \"alcatraz_california\",\n",
        "    \"13526\": \"hofburg_vienna_austria\",\n",
        "    \"1553\": \"berlin_cathedral_germany\",\n",
        "    \"10900\": \"commerzbank_tower_frankfurt\",\n",
        "    \"8063\": \"hagia_sophia_istanbul_turkey\"\n",
        "}\n",
        "\n",
        "keys = list(key_map.keys())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L74DA_E0Wf1b"
      },
      "source": [
        "# Trying to print how many urls are there in each landmark_id.\n",
        "frames = {}\n",
        "\n",
        "for elem in keys:\n",
        "    frame = train_df.loc[train_df[\"landmark_id\"] == elem]\n",
        "    print(elem + \" -> \" + str(frame.shape))\n",
        "    frames[elem] = frame\n",
        "\n",
        "len(frames)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er71zRR9Wf1c"
      },
      "source": [
        "# Downloading images from the urls and loading them in local directory\n",
        "base_directory = \"/content/drive/MyDrive/Landmark_Recognition_Project/downloads\"\n",
        "\n",
        "train_directory = '{}/train'.format(base_directory)\n",
        "test_directory = '{}/test'.format(base_directory)\n",
        "\n",
        "\n",
        "train_count = 1600\n",
        "test_count = 400\n",
        "\n",
        "for key in keys:\n",
        "    train_urls = frames[key][\"url\"][0:train_count].values # [0:1600] urls from the landmark_ids\n",
        "    test_urls = frames[key][\"url\"][train_count:train_count+test_count].values #[1600: 2000] urls from landmark_ids\n",
        "    \n",
        "    # Download training images\n",
        "    for index, url in enumerate(train_urls, start=1):\n",
        "        folder_path = '{}/{}'.format(train_directory, key)\n",
        "        filename = 'image_{}.jpg'.format(index )\n",
        "        file_path = '{}/{}'.format(folder_path, filename)\n",
        "        \n",
        "        \n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print('Image %s already exists. Skipping download.' % file_path)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = urlopen(url)\n",
        "            image_data = response.read()\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Could not download image %s from %s' % (filename, url))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pil_image = Image.open(BytesIO(image_data))\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Failed to parse image %s' % filename)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pil_image_rgb = pil_image.convert('RGB')\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Failed to convert image %s to RGB' % filename)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pil_image_rgb.save(file_path, format='JPEG', quality=90)\n",
        "            print('Success: Saved image %s' % filename)\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Failed to save image %s' % filename)\n",
        "            continue\n",
        "    \n",
        "    # Download testing images\n",
        "    for index, url in enumerate(test_urls, start=1):\n",
        "        folder_path = '{}/{}'.format(test_directory, key)\n",
        "        filename = 'image_{}.jpg'.format(index)\n",
        "        file_path = '{}/{}'.format(folder_path, filename)\n",
        "        \n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print('Image %s already exists. Skipping download.' % file_path)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = urlopen(url)\n",
        "            image_data = response.read()\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Could not download image %s from %s' % (filename, url))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            \n",
        "            pil_image = Image.open(BytesIO(image_data))\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Failed to parse image %s' % filename)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pil_image_rgb = pil_image.convert('RGB')\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Failed to convert image %s to RGB' % filename)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pil_image_rgb.save(file_path, format='JPEG', quality=90)\n",
        "            print('Success: Saved image %s' % file_path)\n",
        "        except Exception as err:\n",
        "            traceback.print_exc()\n",
        "            print('Warning: Failed to save image %s' % filename)\n",
        "            continue\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HplKWbxa1y3y"
      },
      "source": [
        "# Building the input pipeline\n",
        "base_directory = \"/content/drive/MyDrive/Landmark_Recognition_Project/downloads/\"\n",
        "train_directory = '{}train/'.format(base_directory)\n",
        "test_directory = '{}test/'.format(base_directory)\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "# Taking the images from the local directory and spliting 80% for training and 20% for validation\n",
        "train_generator = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_directory,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  labels=\"inferred\",\n",
        "  label_mode=\"int\",\n",
        "  batch_size=batch_size)\n",
        "\n",
        "validation_generator = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_directory,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  labels=\"inferred\",\n",
        "  label_mode=\"int\",\n",
        "  batch_size=batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX8iB5ZY1zBV"
      },
      "source": [
        "# Finding the classes for my model.\n",
        "class_names = train_generator.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSwefsP-1zHu"
      },
      "source": [
        "# Visualizing the data\n",
        "plt.figure(figsize=(20, 20))\n",
        "for images, labels in train_generator.take(1):\n",
        "    for i in range(13):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(key_map[class_names[labels[i]]])\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAHBxhAd1zLq"
      },
      "source": [
        "for image_batch, labels_batch in train_generator:\n",
        "    print(image_batch.shape)\n",
        "    print(labels_batch.shape)\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T71N78dn3y5F"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3DScjtu3zFb"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_generator.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = validation_generator.cache().prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDeuUJid7APt"
      },
      "source": [
        "# Number of classes used for my model\n",
        "num_classes = 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMSr2m8D3zMc"
      },
      "source": [
        "# Building model without using any pre- trained model.\n",
        "model = tf.keras.Sequential([\n",
        "  # First we have to rescale the image size as as CNN accepts values from 0 to 1.\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, activation = 'softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HNmO4L83zO3"
      },
      "source": [
        "# Compiling the model with optimizer 'adam' and loss function as sparse-category-crossentropy\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDl10PhC6ZGa"
      },
      "source": [
        "# Building Model using ResNet50 Pre trained model as the base layer\n",
        "my_new_model = Sequential()\n",
        "my_new_model.add(ResNet50(include_top = False, weights = 'imagenet', pooling = 'avg'))\n",
        "my_new_model.add(Dense(128, activation = 'relu'))\n",
        "my_new_model.add(Dense(128, activation = 'relu'))\n",
        "my_new_model.add(Dense(num_classes, activation = 'softmax'))\n",
        "my_new_model.layers[0].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXX7vjbT6ewo"
      },
      "source": [
        "# Compiling the model with optimizer 'adam' and loss function as sparse-category-crossentropy\n",
        "my_new_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFL4VXs63zax"
      },
      "source": [
        "# Fitting the model to the training data with epochs = 10\n",
        "with_premodel = my_new_model.fit(train_generator,\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=len(validation_generator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tztrC_dt3zUB"
      },
      "source": [
        "# loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(with_premodel.history['loss'], label='train loss')\n",
        "plt.plot(with_premodel.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONX6boKNG39F"
      },
      "source": [
        "# accuracies\n",
        "plt.plot(with_premodel.history['accuracy'], label='train acc')\n",
        "plt.plot(with_premodel.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4y6FhcMvsgz"
      },
      "source": [
        "# Saving the model \n",
        "my_new_model.save('/content/drive/MyDrive/Landmark_Recognition_Project/landmark-recognition-resnet-10epoch-new.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_zRzHCAxI6O"
      },
      "source": [
        "# Loading the model\n",
        "trained_model = tf.keras.models.load_model('/content/drive/MyDrive/Landmark_Recognition_Project/landmark-recognition-resnet-10epoch-new.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoeyTlkryl7q"
      },
      "source": [
        "# Using the newly created model to predict the new url\n",
        "from tensorflow import keras\n",
        "\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
        "path = tf.keras.utils.get_file('Red_sunflower', origin=url)\n",
        "\n",
        "img = keras.preprocessing.image.load_img(\n",
        "    path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = trained_model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "predicted_key = class_names[np.argmax(score)]\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(key_map[predicted_key], 100 * np.max(score))\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbUBQnjBrf6I"
      },
      "source": [
        "# Trying to improve the model with giving 20 epoch values\n",
        "with_premodel_20 = my_new_model.fit(train_generator,\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=20,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=len(validation_generator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlWEUnLxrguU"
      },
      "source": [
        "# loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(with_premodel_20.history['loss'], label='train loss')\n",
        "plt.plot(with_premodel_20.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9CRhhWtrmkw"
      },
      "source": [
        "# accuracies\n",
        "plt.plot(with_premodel_20.history['accuracy'], label='train acc')\n",
        "plt.plot(with_premodel_20.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCFI3hMUCr_E"
      },
      "source": [
        "# Saving the newly improved model\n",
        "my_new_model.save('/content/drive/MyDrive/Landmark_Recognition_Project/landmark-recognition-resnet-20epoch.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7aB-xtZC4yo"
      },
      "source": [
        "# Loading the model\n",
        "trained_model = tf.keras.models.load_model('/content/drive/MyDrive/Landmark_Recognition_Project/landmark-recognition-resnet-20epoch.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pS3UtYmH5at"
      },
      "source": [
        "# For Testing the model used 9 images from my vacation pictures\n",
        "demo_data_dir = pathlib.Path(\"/content/drive/MyDrive/Landmark_Recognition_Project/demo_data/\")\n",
        "demo_list = list(demo_data_dir.glob('*'))\n",
        "image_count = len(demo_list)\n",
        "image_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X70-r8feDCAZ"
      },
      "source": [
        "# Testing the model\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "import numpy as  np\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "results = []\n",
        "\n",
        "for path in demo_list:\n",
        "\n",
        "    # path = tf.keras.utils.get_file('demo', origin=url)\n",
        "\n",
        "    img = keras.preprocessing.image.load_img(\n",
        "        path, target_size=(img_height, img_width)\n",
        "    )\n",
        "    img_array = keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "    # predictions = trained_model.predict(img_array)[0]\n",
        "    # scores = tf.nn.softmax(predictions)\n",
        "\n",
        "    # for prediction, score in list(zip(predictions, scores)):\n",
        "      \n",
        "    #   predicted_key = class_names[np.argmax(score)]\n",
        "    #   print(\"    prediction {} - score {} - key {}\".format(prediction, score, key_map[predicted_key]))\n",
        "\n",
        "\n",
        "    # decoded_predictions = decode_predictions(predictions)\n",
        "    # predict_dict[path] = decoded_predictions\n",
        "    # print(decoded_predictions)\n",
        "\n",
        "    predictions = trained_model.predict(img_array)\n",
        "    score = tf.nn.softmax(predictions[0])\n",
        "    predicted_key = class_names[np.argmax(score)]\n",
        "    confidence =  100 * np.max(score)\n",
        "    results.append((img, key_map[predicted_key], confidence))\n",
        "    print(\n",
        "        \"This image most likely is from {} with a {:.2f} percent confidence.\"\n",
        "        .format(key_map[predicted_key],confidence)\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAYrGPpHueFh"
      },
      "source": [
        "# Visualizing testing data\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for i, (image, label, confidence) in enumerate(results):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image)\n",
        "    title = \"{} ({:.2f}%)\".format(label, confidence)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTeCoDqWTuq6"
      },
      "source": [
        "# Final Demonstration, using a live url from bing search and trying to predict the landmark of that image.\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "import numpy as  np\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "\n",
        "url = \"https://th.bing.com/th/id/OIP.2zrAqdG7Kf6mQ62NxtGpqAHaE9?w=252&h=180&c=7&o=5&dpr=2&pid=1.7\"\n",
        "path = tf.keras.utils.get_file('demo', origin=url)\n",
        "\n",
        "img = keras.preprocessing.image.load_img(\n",
        "      path, target_size=(img_height, img_width)\n",
        "      )\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "\n",
        "predictions = trained_model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "predicted_key = class_names[np.argmax(score)]\n",
        "confidence =  100 * np.max(score)\n",
        "print(\n",
        "      \"This image most likely is from {} with a {:.2f} percent confidence.\"\n",
        "      .format(key_map[predicted_key],confidence)\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}